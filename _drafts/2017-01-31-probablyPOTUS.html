---
layout: single
title: "ProbablyPOTUS: Using Machine Learning to detect tweets written by POTUS"
excerpt_separator: <!--more-->
---
<!--
President Donald Trump has a way with words (that's putting it more nicely than he deserves).  His Twitter feed has managed to
<a href="https://twitter.com/realDonaldTrump/status/811977223326625792">scare the hell out of me</a>, and yet I
can't help but get a good chuckle when he does things like
<a href="https://twitter.com/realDonaldTrump/status/805278955150471168">attack SNL</a> because he can't take a joke.
-->


I've been looking for a project to play with for a while now so that I could officially end my long hiatus
from this blog (devotees will notice the blog's title has been updated and the platform underwent a shiny upgrade).
One of my favorite data science stories this year was an
<a href="http://varianceexplained.org/r/trump-tweets/">analysis</a> done on (then-candidate) Trump's tweets
that showed the crazy tweets tended to come from his personal Andriod phone, while more conventional tweets that
his staff might compose came from other sources.  I was impressed with how cleanly the space of crazy and sane tweets
was divided by which device the tweet came from--you might say the decision function has
<a href="https://en.wikipedia.org/wiki/Support_vector_machine#Linear_SVM">"wide margins"</a>. <br><br>

That got me thinking...what if we didn't know which device the tweet came from, maybe we could use Machine Learning to
predict which device it <i>would</i> have come from, thereby predicting who composed it.
<!--more-->
Why wouldn't we know which
device it came from?  Because Donald Trump was about to become president and there was <em>no way</em> they'd let him
continue to tweet from his insecure personal phone, right? <br><br>

<b>EDIT:</b> It turns out President Trump is defying all common sense and
<a href="https://www.nytimes.com/2017/01/25/us/politics/president-trump-white-house.html?_r=0">
  continuing to tweet with his personal Android.</a>
Should he eventually decide to listen to the 90 bajillion cybersecurity experts who think this is a terrible idea,
then the rest of this blog post will be super relevant. <br><br>

Anyway, this is a fairly well-posed supervised learning problem: using Trump's tweets before he took office, can we
learn a set of rules that classify whether a tweet was sent by his Android or not?  We'll use tweets from before he
took office to build a training set.  The labels will be whether or not the tweet was sent from an Andriod, which we
know maps fairly well to whether the tweet was actually written by Trump.  We can use the text of the tweet and some
other metadata to build features.  Our model will automatically learn a set of rules to determine how the features
map to the class labels. <br>

<h3>Eating your broccoli first</h3> <br>

Before we get to the fun part of modeling, we have to do some engineering work.  First, we need to build up our
training set.  To do this, I'll follow an ELT (Extract, Load, Transform...the order isn't a mistake) framework, using a
PostgreSQL database to store my data for easy retrieving later. <br><br>

For the Extract portion, I need to get my hands on some realDonaldTrump tweets.
Twitter has a REST API that anyone can connect to in order to query tweets, and there's an excellent python
library called <a href="http://www.tweepy.org/">tweepy</a> that abstracts away much of the dirty work so you can
focus on writing python instead of forming HTTP requests.
My <a href="https://github.com/jjardel/probablyPOTUS/blob/master/etl/extract/src/_extractors.py">TweetExtractor</a>
class uses tweepy to query Twitter's API for a few thousand Trump tweets and saves
them to disk. "Load" refers to the act of loading this data from flat files into the database I have
running on Amazon's
EC2.  I tend to keep the Load step simple, not worrying about data types and just bringing everything into the database
as raw text.  Since this is something I do a lot of, I have some
<a href="https://github.com/jjardel/utils/blob/41e09554fc1a7392f00abceb50c1b18dc550c656/db_conn/_db_conn.py">wrappers</a>
that make this task relatively painless for small-ish data sets.  Finally,
<a href="https://github.com/jjardel/probablyPOTUS/tree/master/etl/transform">SQL scripts</a> in the Transform step make
sure that data types are correct in addition to performing some lightweight feature engineering--building features
that might be useful to the model (e.g. how many characters the tweet contains, how many uppercase substrings, the
number of exclamation points, etc.).

<h3>The fun part</h3>

OK, now we're ready to build the model...well, almost.  First we need to work on converting the features into a more
useful form.  We're dealing with both text data (the actual content of the tweets) as well as numerical data
(character counts, # of retweets, etc) so I need to split the feature transformations steps into separate pipelines for
each type of feature (more on that later).<br><br>

Let's start with the text data first.  The first problem to solve is how to represent
unstructured text as a coherent set of features so that we can build a model.
I'll follow a classic Natural Language Processing (NLP) pattern
called the Bag of Words model.  This representation builds up a vocabulary of all the N words in all tweets, and transforms
a single tweet's text into a vector of ones and zeros with length N where each element j indicates whether
a given word is present in the tweet.  We can go one step beyond this simple model by using
Term-frequency--Inverse document frequency (<a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a>) weighting
to assign more weight to words that occur in fewer tweets while down-weighting common words like "the" or "a" that
just add to the noise. <br><br>

This sounds promising, but we're getting ahead of ourselves a bit.  We need to build up a Bag of Words representation,
but we don't have a set of words just yet--we have tweets.  We first need to figure out a way to parse our tweets
to extract words from them.  You might think this is as trivial as <code>str.split(' ')</code>, but there's more to
it than that.  We need to deal with punctuation as well as the fact that tweets are not the same as plain text.
We need to process emojis, hashtags, and 13-year-old-girlspeak.  It's a good thing the powerful NLTK library has a
built-in <a href="http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.casual.TweetTokenizer">TweetTokenizer</a>
class that does all of this for you!  Tokenizers like this are built off of a set of well-established
heuristics that work pretty well out of the box.  Still, I helped it out a little by writing a regex to
<a href="https://github.com/jjardel/probablyPOTUS/blob/master/model/src/_model.py#L75">map all urls</a>
as one distinct token in our vocabulary.


<br> <br> <br>
heuristics

- NLP processing steps:
 -- url standardization
 -- tokenization
 -- n-grams + count vectorization
 -- TF-IDF weighting
- numeric features pre-processing
 -- min/max scaling
- sklearn Pipelines + FeatureUnions
- train/test split
- model fitting, hyperparameter tuning, and k-fold CV
- evaluation on test set (ROC, accuracy)
- serialization


<h3>Deploying a Twitter bot</h3>
 - load serialized model object + transform steps
 - connect to streaming API
 - don't load tweets into DB, perform transformations in memory
 - make a decision & retweet if necessary